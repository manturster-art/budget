import streamlit as st
import pandas as pd
from curl_cffi import requests
from bs4 import BeautifulSoup, SoupStrainer
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
import os

# ğŸ’¡ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦ í•¨ìˆ˜
def authenticate_gdrive_with_secrets():
    gauth = GoogleAuth()

    # Streamlit Secretsì—ì„œ ë°ì´í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ ì„¤ì •
    # ë°•ì‚¬ë‹˜ì˜ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ê°€ ë©”ëª¨ë¦¬ìƒì—ì„œ ë°”ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
    credentials = dict(st.secrets["gdrive"])

    settings = {
        "client_config_backend": "service",
        "service_config": {
            "client_json_dict": credentials, # íŒŒì¼ ê²½ë¡œ ëŒ€ì‹  ë”•ì…”ë„ˆë¦¬(dict) ì „ë‹¬
        }
    }
    gauth.LoadCredentialsFromSettings(settings)
    return GoogleDrive(gauth)

# ğŸ’¡ íŒŒì¼ ì—…ë¡œë“œ/ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def upload_to_gdrive(drive, local_path, folder_id):
    # í´ë” ë‚´ì— ë™ì¼í•œ ì´ë¦„ì˜ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)
    file_list = drive.ListFile({'q': f"'{folder_id}' in parents and trashed=false"}).GetList()
    
    target_file = None
    for file in file_list:
        if file['title'] == os.path.basename(local_path):
            target_file = file
            break
            
    if target_file:
        target_file.SetContentFile(local_path)
    else:
        target_file = drive.CreateFile({'title': os.path.basename(local_path), 'parents': [{'id': folder_id}]})
        target_file.SetContentFile(local_path)
    
    target_file.Upload()

# ---------------------------------------------------------
# ê¸°ë³¸ ì›¹í˜ì´ì§€ ì„¤ì • ë° ìµœì í™” ì„¸ì…˜
# ---------------------------------------------------------
st.set_page_config(page_title="ì§€ë°©ì¬ì •365 í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ê¸°", page_icon="ğŸ›ï¸", layout="wide")
st.title("ğŸ›ï¸ ì§€ë°©ì¬ì •365 ì„¸ë¶€ì‚¬ì—… ë°ì´í„° ìˆ˜ì§‘ê¸° (ì´ˆê³ ì†/ë§¤í•‘ í†µí•©ë³¸)")
st.markdown("""
ì´ ì›¹ í”„ë¡œê·¸ë¨ì€ ì§€ë°©ì¬ì •365 ë‚´ë¶€ APIì™€ ë¡œì»¬ ì˜ˆì‚° ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬, í•µì‹¬ ì‚¬ì—…ì˜ í…ìŠ¤íŠ¸ë§Œ ì´ˆê³ ì†ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
""")

session_step1 = requests.Session(impersonate="chrome")
session_step2 = requests.Session(impersonate="chrome")

# ---------------------------------------------------------
# ğŸ’¡ [ìƒˆë¡œ ì¶”ê°€ëœ ë§ŒëŠ¥ í—¬í¼ í•¨ìˆ˜] ì–´ë–¤ CSV/ì—‘ì…€ì´ë“  ì™„ë²½í•˜ê²Œ ì½ì–´ëƒ…ë‹ˆë‹¤.
# ---------------------------------------------------------
def load_safe_df(file_obj):
    file_obj.seek(0) # í¬ì¸í„°(ì±…ê°ˆí”¼)ë¥¼ íŒŒì¼ ë§¨ ì²˜ìŒìœ¼ë¡œ ì´ˆê¸°í™”
    ext = file_obj.name.lower() # ëŒ€ë¬¸ì .CSVë¥¼ ì†Œë¬¸ìë¡œ í†µì¼
    
    if ext.endswith('.csv'):
        # ê³µê³µë°ì´í„°ì—ì„œ ì£¼ë¡œ ì“°ì´ëŠ” 3ê°€ì§€ ì¸ì½”ë”©ê³¼ í—¤ë” ìœ„ì¹˜(0, 1)ë¥¼ ëª¨ë‘ ì‹œë„
        for enc in ['utf-8', 'cp949', 'euc-kr']:
            for h in [0, 1]:
                try:
                    file_obj.seek(0)
                    df = pd.read_csv(file_obj, header=h, encoding=enc)
                    df.columns = df.columns.str.strip() # ì»¬ëŸ¼ëª… ê³µë°± ì‹¹ ì œê±°
                    if any(col in df.columns for col in ['ìì¹˜ë‹¨ì²´ì½”ë“œ', 'ì§€ì—­', 'ì„¸ë¶€ì‚¬ì—…ëª…']):
                        return df
                except Exception:
                    continue
        file_obj.seek(0)
        return pd.read_csv(file_obj, encoding='cp949', on_bad_lines='skip')
    else:
        for h in [0, 1]:
            try:
                file_obj.seek(0)
                df = pd.read_excel(file_obj, header=h)
                df.columns = df.columns.str.strip()
                if any(col in df.columns for col in ['ìì¹˜ë‹¨ì²´ì½”ë“œ', 'ì§€ì—­', 'ì„¸ë¶€ì‚¬ì—…ëª…']):
                    return df
            except Exception:
                continue
        file_obj.seek(0)
        return pd.read_excel(file_obj)

# ---------------------------------------------------------
# [1ë‹¨ê³„ í—¬í¼ í•¨ìˆ˜] API ë°ì´í„° ìˆ˜ì§‘
# ---------------------------------------------------------
def fetch_region_data(region, year, api_key):
    laf_cd = str(region['ìì¹˜ë‹¨ì²´ì½”ë“œ'])
    laf_nm = str(region['ìì¹˜ë‹¨ì²´ëª…'])
    api_url = "https://www.lofin365.go.kr/lf/hub/QWGJK"
    
    region_data = []
    pIndex = 1
    
    while True:
        payload = {
            'Key': api_key, 'Type': 'json', 'pIndex': pIndex, 'pSize': 1000,
            'fyr': year, 'laf_cd': laf_cd, 'exe_ymd': f"{year}1231"
        }
        try:
            response = session_step1.get(api_url, params=payload, timeout=15)
            data = response.json()
            
            try:
                items = data['QWGJK'][1]['row']
            except (KeyError, IndexError):
                items = []
                
            if not items: break
                
            for item in items:
                region_data.append({
                    'íšŒê³„ì—°ë„': item.get('fyr'),
                    'ì§€ìì²´ì½”ë“œ': item.get('laf_cd'),
                    'ì§€ìì²´ëª…': item.get('laf_hg_nm'),
                    'ì„¸ë¶€ì‚¬ì—…ì½”ë“œ': item.get('dbiz_cd'),
                    'ì„¸ë¶€ì‚¬ì—…ëª…': item.get('dbiz_nm'),
                    'ì˜ˆì‚°í˜„ì•¡': item.get('bdg_cash_amt', 0),
                    'ì§€ì¶œì•¡': item.get('ep_amt', 0)
                })
            
            if len(items) < 1000: break
            pIndex += 1
            time.sleep(0.01) 
            
        except Exception:
            break
            
    return region_data, laf_nm

# ---------------------------------------------------------
# UI íƒ­ ì„¤ì •
# ---------------------------------------------------------
tab1, tab2 = st.tabs(["[1ë‹¨ê³„] ì‚¬ì—…ëª©ë¡ ì¶”ì¶œ ë° ë§¤í•‘", "[2ë‹¨ê³„] ì‚¬ì—…ê°œìš” ì¶”ì¶œ"])

with tab1:
    st.header("1. API ì‚¬ì—…ëª©ë¡ ì¶”ì¶œ ë° ë¡œì»¬ ë°ì´í„° ë³‘í•©")
    
    col1, col2 = st.columns(2)
    with col1:
        api_key = st.text_input("API ì¸ì¦í‚¤ (Decoding Key)", type="password")
    with col2:
        years_list = [str(y) for y in range(2016, 2026)]
        target_year = st.selectbox("ì¡°íšŒí•  íšŒê³„ì—°ë„", years_list, index=len(years_list)-3)
        
    region_file = st.file_uploader("ğŸ—ºï¸ [í•„ìˆ˜] ì§€ì—­ì½”ë“œ íŒŒì¼ ì—…ë¡œë“œ (CSV/Excel)", type=['csv', 'xlsx'])
    local_budget_file = st.file_uploader("ğŸ“Š [í•„ìˆ˜] ë¡œì»¬ ì˜ˆì‚°í˜„í™© íŒŒì¼ ì—…ë¡œë“œ (CSV/Excel)", type=['csv', 'xlsx'])
    
    selected_sido = []
    df_region = pd.DataFrame()
    
    if region_file is not None:
        try:
            # ğŸ’¡ ë§ŒëŠ¥ í•¨ìˆ˜ë¡œ ì—ëŸ¬ ì—†ì´ íŒŒì¼ ë¡œë“œ!
            df_region = load_safe_df(region_file)
            
            if 'ì§€ì—­' in df_region.columns:
                unique_sido = df_region['ì§€ì—­'].dropna().unique().tolist()
                selected_sido = st.multiselect("ğŸ“ ìˆ˜ì§‘í•  ê´‘ì—­ ë‹¨ìœ„ ì„ íƒ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)", unique_sido, default=unique_sido)
            else:
                st.warning("ì—…ë¡œë“œëœ íŒŒì¼ì— 'ì§€ì—­' ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ì§€ìì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    if st.button("ğŸš€ 1ë‹¨ê³„ ì¶”ì¶œ ë° ë§¤í•‘ ì‹œì‘", key="btn_step1"):
        if not api_key or region_file is None or local_budget_file is None:
            st.error("API ì¸ì¦í‚¤, ì§€ì—­ì½”ë“œ íŒŒì¼, ë¡œì»¬ ì˜ˆì‚°í˜„í™© íŒŒì¼ì„ ëª¨ë‘ í™•ì¸í•´ì£¼ì„¸ìš”!")
        else:
            with st.spinner("API ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤..."):
                df_region_filtered = df_region[df_region['ì§€ì—­'].isin(selected_sido)] if selected_sido else df_region
                unique_regions = df_region_filtered[['ìì¹˜ë‹¨ì²´ì½”ë“œ', 'ìì¹˜ë‹¨ì²´ëª…']].drop_duplicates().to_dict('records')
                
                target_list = []
                prog_bar_1 = st.progress(0)
                status_1 = st.empty()
                completed_count = 0
                
                with ThreadPoolExecutor(max_workers=10) as executor:
                    futures = [executor.submit(fetch_region_data, region, target_year, api_key) for region in unique_regions]
                    for future in as_completed(futures):
                        result_data, region_name = future.result()
                        if result_data:
                            target_list.extend(result_data)
                        completed_count += 1
                        prog_bar_1.progress(int((completed_count / len(unique_regions)) * 100))
                        status_1.text(f"ğŸš€ API ìˆ˜ì§‘ ì¤‘... [{completed_count}/{len(unique_regions)}]")
                        time.sleep(0.01)
                
                if target_list:
                    # API ë°ì´í„° ì •ë¦¬
                    df_api = pd.DataFrame(target_list).drop_duplicates(subset=['íšŒê³„ì—°ë„', 'ì§€ìì²´ì½”ë“œ', 'ì„¸ë¶€ì‚¬ì—…ì½”ë“œ'])
                    df_api['ì§€ìì²´ëª…'] = df_api['ì§€ìì²´ëª…'].astype(str).str.strip()
                    df_api['ì„¸ë¶€ì‚¬ì—…ëª…'] = df_api['ì„¸ë¶€ì‚¬ì—…ëª…'].astype(str).str.strip()
                    
                    status_1.text("ğŸ’¡ ë¡œì»¬ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘... (ê³ ì† ì—°ì‚°)")
                    
                    # ğŸ’¡ ë§ŒëŠ¥ í•¨ìˆ˜ë¡œ ë¡œì»¬ ì˜ˆì‚° íŒŒì¼ ë¡œë“œ
                    df_local = load_safe_df(local_budget_file)
                    
                    df_local = df_local.dropna(subset=['ì§€ì—­', 'ìì¹˜ë‹¨ì²´', 'ì„¸ë¶€ì‚¬ì—…ëª…'])
                    df_local['ì§€ìì²´ëª…'] = (df_local['ì§€ì—­'].astype(str).str.strip() + df_local['ìì¹˜ë‹¨ì²´'].astype(str).str.strip())
                    df_local['ì„¸ë¶€ì‚¬ì—…ëª…'] = df_local['ì„¸ë¶€ì‚¬ì—…ëª…'].astype(str).str.strip()
                    
                    # ì¼ë°˜ê³µê³µí–‰ì •, ê¸°íƒ€, ì˜ˆë¹„ë¹„ ì™„ë²½ í•„í„°ë§(ì¼ìƒê²½ë¹„ ë…¸ì´ì¦ˆ ì œê±°)
                    df_local = df_local[~df_local['ë¶„ì•¼'].astype(str).str.contains('ì¼ë°˜ê³µê³µí–‰ì •|ì¼ë°˜í–‰ì •|ê¸°íƒ€|ì˜ˆë¹„ë¹„', na=False)]
                    
                    status_1.text("ğŸ’¡ ì¤‘ë³µ ì‚¬ì—… ì••ì¶• ì¤‘... (ë³‘ëª© ìµœì í™”)")
                    
                    # ğŸš€ ê³ ì† ì—°ì‚° (drop_duplicates í›„ unique ì‚¬ìš©)
                    df_local_sub = df_local[['ì§€ìì²´ëª…', 'ì„¸ë¶€ì‚¬ì—…ëª…', 'íšŒê³„', 'ë¶„ì•¼', 'ë¶€ë¬¸']].drop_duplicates()
                    df_local_sub[['íšŒê³„', 'ë¶„ì•¼', 'ë¶€ë¬¸']] = df_local_sub[['íšŒê³„', 'ë¶„ì•¼', 'ë¶€ë¬¸']].fillna('').astype(str)
                    
                    df_local_agg = df_local_sub.groupby(['ì§€ìì²´ëª…', 'ì„¸ë¶€ì‚¬ì—…ëª…'], as_index=False).agg({
                        'íšŒê³„': lambda x: ', '.join([i for i in x.unique() if i]),
                        'ë¶„ì•¼': lambda x: ', '.join([i for i in x.unique() if i]),
                        'ë¶€ë¬¸': lambda x: ', '.join([i for i in x.unique() if i])
                    })
                    
                    status_1.text("ğŸ’¡ ìµœì¢… 1:1 ê²°í•© ì¤‘...")
                    
                    df_step1 = pd.merge(df_api, df_local_agg, on=['ì§€ìì²´ëª…', 'ì„¸ë¶€ì‚¬ì—…ëª…'], how='inner')
                    df_step1.to_csv(f"[ìë™ì €ì¥]_1ë‹¨ê³„_ëª©ë¡_{target_year}.csv", index=False, encoding='utf-8-sig')
                    
                    status_1.text("âœ… ìˆ˜ì§‘ ë° ë§¤í•‘ ì™„ë£Œ!")
                    st.success(f"ğŸ‰ 'ì¼ë°˜ê³µê³µí–‰ì •' ì œì™¸ ì™„ë²½ ë§¤í•‘! ì´ {len(df_step1)}ê±´ ì¶”ì¶œ ì™„ë£Œ.")
                    st.dataframe(df_step1[['ì§€ìì²´ëª…', 'ì„¸ë¶€ì‚¬ì—…ëª…', 'íšŒê³„', 'ë¶„ì•¼', 'ë¶€ë¬¸']].head(10)) 
                    
                    csv_step1 = df_step1.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    region_tag = "ì „ì²´" if len(selected_sido) > 3 else "_".join(selected_sido)
                    st.download_button(
                        label="ğŸ“¥ 1ë‹¨ê³„ ìµœì¢… ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                        data=csv_step1,
                        file_name=f"target_list_mapped_{region_tag}_{target_year}.csv",
                        mime="text/csv"
                    )

# ---------------------------------------------------------
# [2ë‹¨ê³„] ì‚¬ì—…ê°œìš” í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¡œì§ (ìµœì í™” lxml ë²„ì „)
# ---------------------------------------------------------
def extract_clean_text(html_text, target_keyword):
    only_body = SoupStrainer('body')
    soup = BeautifulSoup(html_text, 'lxml', parse_only=only_body)
    raw_text = soup.get_text(separator='|', strip=True)
    parts = raw_text.split('|')
    
    stop_words = ['ì‚¬ì—…ëª©ì ', 'ì‚¬ì—…ê¸°ê°„', 'ì´ì‚¬ì—…ë¹„', 'ì‚¬ì—…ê·œëª¨', 'ì‚¬ì—…ë‚´ìš©', 'ì§€ì›í˜•íƒœ', 'ì§€ì›ì¡°ê±´', 'ì‚¬ì—…ìœ„ì¹˜', 'ì‹œí–‰ì£¼ì²´', 'ì¶”ì§„ê·¼ê±°', 'ì¶”ì§„ê²½ìœ„', 'ì¶”ì§„ê³„íš', 'ì†Œìš”ì¬ì›', 'ì›”ë³„ë°°ì •ì•¡', 'ê³¼ê±°ì§‘í–‰í˜„í™©', 'ì§€ì¶œí˜„í™©']
    extracted, found = [], False
    
    for piece in parts:
        piece = piece.strip()
        if not piece: continue
        is_stop_word = any(piece.replace(" ", "").startswith(sw) for sw in stop_words)
        if not found:
            if target_keyword.replace(" ", "") in piece.replace(" ", ""):
                found = True
                content = piece.split(target_keyword)[-1].replace(":", "").replace("ï¼š", "").strip()
                if content: extracted.append(content)
        else:
            if is_stop_word: break
            if piece not in [':', 'ï¼š', '-', '+']: extracted.append(piece)
    return " ".join(extracted).strip()

def fetch_text_data(row):
    year = str(row.get('íšŒê³„ì—°ë„', row.get('fyr')))
    laf_cd = str(row.get('ì§€ìì²´ì½”ë“œ', row.get('lafCd', row.get('laf_cd'))))
    dbiz_cd = str(row.get('ì„¸ë¶€ì‚¬ì—…ì½”ë“œ', row.get('dbizCd', row.get('dbiz_cd'))))
    
    url = "https://www.lofin365.go.kr/lf/lnncGramStst/laf/exeSvi/retvDtlsBybsnAneSituDts.do"
    payload = {
        'menuUrl': '/lf/lnncGramStst/laf/exeSvi/retvDtlsBybsnAneSituDts.do',
        'menuNm': 'ì„¸ë¶€ì‚¬ì—…ë³„ ì„¸ì¶œí˜„í™© ìƒì„¸', 'menuParaCn': 'STST', 'menuId': 'LF3120204',
        'uprMenuId': 'LF3120202', 'sysDvCd': '', 'logReg': 'true',
        'dbizCd': dbiz_cd, 'lafCd': laf_cd, 'fyr': year, 'inqYmd': f"{year}1231"
    }
    
    try:
        response = session_step2.post(url, data=payload, timeout=20) 
        return {
            'íšŒê³„ì—°ë„': year, 'ì§€ìì²´ì½”ë“œ': laf_cd, 'ì„¸ë¶€ì‚¬ì—…ì½”ë“œ': dbiz_cd,
            'ì‚¬ì—…ëª©ì ': extract_clean_text(response.text, 'ì‚¬ì—…ëª©ì '),
            'ì‚¬ì—…ê¸°ê°„': extract_clean_text(response.text, 'ì‚¬ì—…ê¸°ê°„'),
            'ì‚¬ì—…ë‚´ìš©': extract_clean_text(response.text, 'ì‚¬ì—…ë‚´ìš©'),
            'ì¶”ì§„ê³„íš': extract_clean_text(response.text, 'ì¶”ì§„ê³„íš')
        }
    except Exception:
        return None

with tab2:
    st.header("2. ì‚¬ì—…ê°œìš” í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìŠ¤ë§ˆíŠ¸ ì´ì–´í•˜ê¸° ì§€ì›)")
    st.info("ğŸ’¡ ë©ˆì·„ì„ ê²½ìš°, í´ë”ì— ìˆëŠ” `[ìë™ì €ì¥]_2ë‹¨ê³„...csv` íŒŒì¼ì„ ì—¬ê¸°ì— ì˜¬ë¦¬ë©´ ë‚¨ì€ ê²ƒë§Œ ì´ì–´ì„œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    uploaded_file = st.file_uploader("ğŸ“‚ 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë˜ëŠ” [ìë™ì €ì¥] ë°±ì—… íŒŒì¼ ì—…ë¡œë“œ", type=['csv'])
    
    if uploaded_file is not None:
        df_uploaded = load_safe_df(uploaded_file)
        st.write(f"ì´ **{len(df_uploaded)}**ê±´ì˜ ë°ì´í„°ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
        
        # ğŸ’¡ [ìŠ¤ë§ˆíŠ¸ ì´ì–´í•˜ê¸° ê°ì§€ ë¡œì§]
        if 'ì‚¬ì—…ë‚´ìš©' not in df_uploaded.columns:
            # ì²˜ìŒ ì‹œì‘í•˜ëŠ” ê²½ìš° ë¹ˆ ì»¬ëŸ¼ ìƒì„±
            df_uploaded['ì‚¬ì—…ëª©ì '] = None
            df_uploaded['ì‚¬ì—…ê¸°ê°„'] = None
            df_uploaded['ì‚¬ì—…ë‚´ìš©'] = None
            df_uploaded['ì¶”ì§„ê³„íš'] = None
            
        done_count = df_uploaded['ì‚¬ì—…ë‚´ìš©'].notna().sum()
        todo_count = len(df_uploaded) - done_count
        
        if done_count > 0:
            st.success(f"ğŸ’¾ ì´ì–´í•˜ê¸° ëª¨ë“œ ê°ì§€ë¨! (ì´ë¯¸ ì™„ë£Œ: {done_count}ê±´ / ë‚¨ì€ ì‘ì—…: {todo_count}ê±´)")
        
        if st.button("ğŸš€ 2ë‹¨ê³„ í…ìŠ¤íŠ¸ ë³‘ë ¬ ì¶”ì¶œ ì‹œì‘"):
            target_records = df_uploaded.to_dict('records')
            
            # ğŸ’¡ ìˆ˜ì§‘í•´ì•¼ í•  í–‰(ë¹„ì–´ìˆëŠ” í–‰)ë§Œ í•„í„°ë§í•˜ê³  ì¸ë±ìŠ¤ ì¶”ì 
            records_to_fetch = [
                (idx, row) for idx, row in enumerate(target_records) 
                if pd.isna(row.get('ì‚¬ì—…ë‚´ìš©')) or str(row.get('ì‚¬ì—…ë‚´ìš©')).strip() in ["", "None"]
            ]
            
            if not records_to_fetch:
                st.success("ğŸ‰ ë‚¨ì€ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤! ëª¨ë‘ ìˆ˜ì§‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.stop()
                
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ì•ì„œ ë§ì”€ë“œë¦° ëŒ€ë¡œ ì†ë„ë¥¼ ìœ„í•´ 20~30ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì‚¬ìš©í•˜ì…”ë„ ë©ë‹ˆë‹¤.
            with ThreadPoolExecutor(max_workers=20) as executor:
                # ë¯¸ë˜ ê°ì²´ì— ì¸ë±ìŠ¤(idx)ë¥¼ ë§¤í•‘í•˜ì—¬ ì›ë³¸ ë°ì´í„°ì˜ ì œìë¦¬ì— ê½‚ì•„ ë„£ê¸°
                future_to_idx = {executor.submit(fetch_text_data, row): idx for idx, row in records_to_fetch}
                
                completed_in_this_run = 0
                total_to_fetch = len(records_to_fetch)
                
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    result = future.result()
                    
                    if result:
                        # ì›ë³¸ ë ˆì½”ë“œì˜ ë¹ˆì¹¸ì— ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ì¦‰ì‹œ ë®ì–´ì“°ê¸° (Merge ì¤‘ë³µ ì˜¤ë¥˜ ì›ì²œ ì°¨ë‹¨)
                        target_records[idx]['ì‚¬ì—…ëª©ì '] = result['ì‚¬ì—…ëª©ì ']
                        target_records[idx]['ì‚¬ì—…ê¸°ê°„'] = result['ì‚¬ì—…ê¸°ê°„']
                        target_records[idx]['ì‚¬ì—…ë‚´ìš©'] = result['ì‚¬ì—…ë‚´ìš©']
                        target_records[idx]['ì¶”ì§„ê³„íš'] = result['ì¶”ì§„ê³„íš']
                        
                    completed_in_this_run += 1
                                        
                    # ğŸ’¡ 1. [í™”ë©´ ì—…ë°ì´íŠ¸]: 1ê±´ ì™„ë£Œë  ë•Œë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ë¥  ê°±ì‹  (ì†ë„ ì €í•˜ ì—†ìŒ)
                    progress = int((completed_in_this_run / total_to_fetch) * 100)
                    progress_bar.progress(progress)
                    status_text.text(f"ğŸš€ ë°ì´í„° ì¶”ì¶œ ì¤‘... ({completed_in_this_run} / {total_to_fetch} ê±´ ì™„ë£Œ)")
                    
                    # ğŸ’¡ 2. [ë¬¼ë¦¬ì  ë°±ì—… ì €ì¥]: ë””ìŠ¤í¬ I/O ë³‘ëª©(ì†ë„ ì €í•˜)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 1000ê±´ë§ˆë‹¤ 1ë²ˆì”©ë§Œ ëª°ì•„ì„œ íŒŒì¼ ì €ì¥
                    if completed_in_this_run % 1000 == 0 or completed_in_this_run == total_to_fetch:
                        local_path = "[ìë™ì €ì¥]_2ë‹¨ê³„_í…ìŠ¤íŠ¸ì¶”ì¶œ_ë°±ì—….csv"
                        
                        # 1. ë¡œì»¬ í•˜ë“œë””ìŠ¤í¬ì— ë¨¼ì € ì €ì¥ (ì•ˆì „ ì¥ì¹˜)
                        pd.DataFrame(target_records).to_csv(local_path, index=False, encoding='utf-8-sig')
                        status_text.text(f"ğŸš€ {completed_in_this_run}/{total_to_fetch} ì™„ë£Œ - ğŸ’¾ ë¡œì»¬ ë°±ì—… ì™„ë£Œ!")

                        # 2. Google Drive ì‹¤ì‹œê°„ ì—…ë¡œë“œ (Secrets ì¸ì¦ ë°©ì‹)
                        try:
                            # âš ï¸ ìƒë‹¨ì— ì •ì˜í•œ 'authenticate_gdrive_with_secrets' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                            drive = authenticate_gdrive_with_secrets() 
                            
                            # âš ï¸ 'ë°•ì‚¬ë‹˜ì˜_í´ë”_ID' ë¶€ë¶„ì„ ì‹¤ì œ êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë” IDë¡œ ê¼­ ìˆ˜ì •í•˜ì„¸ìš”!
                            upload_to_gdrive(drive, local_path, "3NekjB0SM39VhTsw74lcTMGyPREOEDEU")
                            
                            status_text.text(f"ğŸš€ {completed_in_this_run}/{total_to_fetch} ì™„ë£Œ - â˜ï¸ Google Drive ì—…ë¡œë“œ ì„±ê³µ!")
                        except Exception as e:
                            # ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ ë¡œì»¬ ì €ì¥ì€ ì™„ë£Œëœ ìƒíƒœì´ë¯€ë¡œ ê²½ê³ ë§Œ í‘œì‹œ
                            st.warning(f"âš ï¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¡œì»¬ ì €ì¥ì€ ì•ˆì „í•¨): {e}")                        

                    time.sleep(0.01)
            
            status_text.text("âœ… ì¶”ì¶œ ì™„ë£Œ!")
            st.success("ğŸ‰ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì™„ë²½í•˜ê²Œ ëë‚¬ìŠµë‹ˆë‹¤!")
            
            df_final = pd.DataFrame(target_records)
            st.dataframe(df_final[['ì§€ìì²´ëª…', 'ì„¸ë¶€ì‚¬ì—…ëª…', 'ì‚¬ì—…ë‚´ìš©']].head(5))
            
            csv_final = df_final.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                label="ğŸ“¥ 2ë‹¨ê³„ ìµœì¢… í†µí•© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
                data=csv_final,
                file_name="budget_text_final_result.csv",
                mime="text/csv"
            )
